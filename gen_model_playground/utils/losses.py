import torch
import torch.nn.functional as F
from torch import nn
class model():
    def __init__(self, in_features, out_features, num_blocks, hidden_features, cond_features=None, spectral=False, batch_norm=False, residual=False, time_features=None, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.num_blocks = num_blocks
        self.hidden_features = hidden_features
        self.cond_features = cond_features
        self.spectral = spectral
        self.batch_norm = batch_norm
        self.residual = residual
        self.time_features = time_features
        self.bias = bias
        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(in_features, hidden_features, bias=bias))
        if batch_norm:
            self.layers.append(nn.BatchNorm1d(hidden_features))
        self.layers.append(nn.ReLU())
        for i in range(num_blocks - 1):
            self.layers.append(nn.Linear(hidden_features, hidden_features, bias=bias))
            if batch_norm:
                self.layers.append(nn.BatchNorm1d(hidden_features))
            self.layers.append(nn.ReLU())
        self.layers.append(nn.Linear(hidden_features, out_features, bias=bias))
        if time_features is not None:
            self.time = nn.Linear(time_features, hidden_features, bias=bias)
        if cond_features is not None:
            self.cond = nn.Linear(cond_features, hidden_features, bias=bias)
        if spectral:
            self.apply(self.spectral_norm)
def least_squares(y_real, y_fake, critic):
    """
    Implements the Least Squares GAN (LSGAN) loss function.

    Args:
        y_real (torch.Tensor): Discriminator output for real data.
        y_fake (torch.Tensor): Discriminator output for fake data.
        critic (bool): Indicates if the function is used for the discriminator (True) or generator (False).

    Returns:
        torch.Tensor: The calculated loss value.
    """
    if critic:
        return (F.mse_loss(y_real, torch.ones_like(y_real)) + F.mse_loss(y_fake, torch.zeros_like(y_fake)))
    else:
        return 0.5 * F.mse_loss(y_fake, torch.ones_like(y_fake))

def wasserstein(y_real, y_fake, critic):
    """
    Calculates the Wasserstein loss, used in Wasserstein GANs (WGANs).

    Args:
        y_real (torch.Tensor): Discriminator output for real data.
        y_fake (torch.Tensor): Discriminator output for fake data.
        critic (bool): Indicates if the function is used for the discriminator (True) or generator (False).

    Returns:
        torch.Tensor: The calculated loss value.
    """
    if critic:
        return (-y_real + y_fake).mean()
    else:
        return (-y_fake).mean()

def non_saturating(y_real, y_fake, critic):
    """
    Computes the Non-Saturating GAN loss using binary cross-entropy.

    Args:
        y_real (torch.Tensor): Discriminator output for real data.
        y_fake (torch.Tensor): Discriminator output for fake data.
        critic (bool): Indicates if the function is used for the discriminator (True) or generator (False).

    Returns:
        torch.Tensor: The calculated loss value.
    """
    if critic:
        return (F.binary_cross_entropy_with_logits(y_real, torch.ones_like(y_real)) + F.binary_cross_entropy_with_logits(y_fake, torch.zeros_like(y_fake))).mean()
    else:
        return F.binary_cross_entropy_with_logits(y_fake, torch.ones_like(y_fake)).mean()

def gradient_penalty(x, fake, discriminator, cond = None, GP=1):
    """
    Calculates the gradient penalty for the discriminator in WGANs, enforcing the Lipschitz constraint.

    Args:
        x (torch.Tensor): Real data.
        fake (torch.Tensor): Fake data generated by the generator.
        discriminator (nn.Module): The discriminator network.
        GP (float): desired lipschitz constant (default: 1), evidence that 0 performs better

    Returns:
        torch.Tensor: The calculated gradient penalty.
    """
    alpha = torch.rand(x.shape[0], 1, device=x.device)
    alpha = alpha.expand_as(x)
    interpolated = alpha * x + (1 - alpha) * fake
    interpolated.requires_grad_(True)
    y_pred = discriminator(interpolated, cond)

    gradients = torch.autograd.grad(
        outputs=y_pred,
        inputs=interpolated,
        grad_outputs=torch.ones_like(y_pred),
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    gradients = gradients.flatten(start_dim=1)
    gradients_norm = gradients.norm(2, dim=1)
    return (torch.nn.functional.relu(gradients_norm - GP) ** 2).mean()


